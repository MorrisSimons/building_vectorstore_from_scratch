{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05882bce",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679f6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y -q gensim\n",
    "!pip install -q gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b45435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "import heapq\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np\n",
    "from numpy import float32 as FLOAT_TYPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9541e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parameters\n",
    "VECTOR_SIZE = 100\n",
    "WINDOW      = 5\n",
    "MIN_COUNT   = 5\n",
    "WORKERS     = 4\n",
    "MAX_WORDS_IN_BATCH = 10000\n",
    "#MAX_WORDS From https://github.com/piskvorky/gensim/blob/develop/gensim/models/word2vec_inner.pyx#L27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78484fb5",
   "metadata": {},
   "source": [
    "### CustomLineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c539367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from gensim import utils\n",
    "\n",
    "class CustomLineSentence:\n",
    "    def __init__(self, source):\n",
    "        self.source = source\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through the lines in the source.\"\"\"\n",
    "        with utils.open(self.source, 'rb') as fin:\n",
    "            for line in itertools.islice(fin, None):\n",
    "                line = utils.to_unicode(line).split()\n",
    "                i = 0\n",
    "                while i < len(line):\n",
    "                    yield line[i: i + MAX_WORDS_IN_BATCH]\n",
    "                    i += MAX_WORDS_IN_BATCH\n",
    "\n",
    "class Heapitem(namedtuple('Heapitem', 'count, index, left, right')):\n",
    "    def __lt__(self, other):\n",
    "        return self.count < other.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407a4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the text8 corpus as a stream of sentences\n",
    "sentences = CustomLineSentence('data/text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_vocab_item(word, count, min_count):\n",
    "    default_res = count >= min_count\n",
    "    return default_res\n",
    "\n",
    "def _build_heap(wv):\n",
    "    heap = list(Heapitem(wv.get_vecattr(i, 'count'), i, None, None) for i in range(len(wv.index_to_key)))\n",
    "    heapq.heapify(heap)\n",
    "    for i in range(len(wv) - 1):\n",
    "        min1, min2 = heapq.heappop(heap), heapq.heappop(heap)\n",
    "        heapq.heappush(\n",
    "            heap, Heapitem(count=min1.count + min2.count, index=i + len(wv), left=min1, right=min2)\n",
    "        )\n",
    "    return heap\n",
    "\n",
    "def _assign_binary_codes(wv):\n",
    "    heap = _build_heap(wv)\n",
    "    \n",
    "    max_depth = 0\n",
    "    stack = [(heap[0], [], [])]\n",
    "    while stack:\n",
    "        node, codes, points = stack.pop()\n",
    "        if node[1] < len(wv):  # node[1] = index\n",
    "            # leaf node => store its path from the root\n",
    "            k = node[1]\n",
    "            wv.set_vecattr(k, 'code', codes)\n",
    "            wv.set_vecattr(k, 'point', points)\n",
    "            # node.code, node.point = codes, points\n",
    "            max_depth = max(len(codes), max_depth)\n",
    "        else:\n",
    "            # inner node => continue recursion\n",
    "            points = np.array(list(points) + [node.index - len(wv)], dtype=np.uint32)\n",
    "            stack.append((node.left, np.array(list(codes) + [0], dtype=np.uint8), points))\n",
    "            stack.append((node.right, np.array(list(codes) + [1], dtype=np.uint8), points))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e66dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWord2Vec:\n",
    "     def __init__(self, sentences=None, vector_size=100, window=5, min_count=5, workers=3):\n",
    "\n",
    "          corpus_iterable = sentences\n",
    "\n",
    "          self.sentences = sentences\n",
    "          self.vector_size = vector_size\n",
    "          self.window = window\n",
    "          self.min_count = min_count\n",
    "          self.workers = workers\n",
    "\n",
    "          # from Word2Vec parameters\n",
    "          self.shrink_windows=True\n",
    "          self.compute_loss=False\n",
    "          self.sorted_vocab=1\n",
    "          self.null_word=0\n",
    "          self.epochs=5\n",
    "          self.hashfxn=hash\n",
    "          self.cbow_mean=1\n",
    "          self.ns_exponent=0.75\n",
    "          self.negative=5\n",
    "          self.hs=0\n",
    "          self.sg=0\n",
    "          self.min_alpha=0.0001\n",
    "          self.seed=1\n",
    "          self.sample=0.001\n",
    "          self.alpha=0.025\n",
    "          \n",
    "          # normal init valuse\n",
    "          self.train_count = 0\n",
    "          self.total_train_time = 0\n",
    "          self.running_training_loss = 0\n",
    "          self.corpus_count = 0\n",
    "          self.corpus_total_words = 0\n",
    "\n",
    "\n",
    "          if not hasattr(self, 'wv'):  # set unless subclass already set (eg: FastText)\n",
    "            self.wv = KeyedVectors(vector_size)\n",
    "          # EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\n",
    "          # advanced users should directly resize/adjust as desired after any vocab growth\n",
    "          self.wv.vectors_lockf = np.ones(1, dtype=FLOAT_TYPE)  # 0.0 values suppress word-backprop-updates; 1.0 allows\n",
    "\n",
    "          # Main part build vocab\n",
    "          self.build_vocab(corpus_iterable=corpus_iterable, corpus_file=None, trim_rule=None)\n",
    "\n",
    "\n",
    "     def scan_vocab(self, sentences=None, corpus_file=None):\n",
    "          \"\"\"Scan the corpus to determine the vocabulary size and word frequencies.\"\"\"\n",
    "          if corpus_file:\n",
    "               sentences = CustomLineSentence(corpus_file) # potentially this could be removed - TODO: test this later\n",
    "\n",
    "          sentence_no = -1         # start from -1 to count the first sentence as 0 in \n",
    "          total_words = 0          # \n",
    "          vocab = defaultdict(int)\n",
    "\n",
    "          for sentence_no, sentence in enumerate(sentences):\n",
    "            for word in sentence:\n",
    "                vocab[word] += 1\n",
    "            total_words += len(sentence)\n",
    "\n",
    "          corpus_count = sentence_no + 1\n",
    "          self.raw_vocab = vocab\n",
    "\n",
    "          return total_words, corpus_count\n",
    "\n",
    "\n",
    "     def add_null_word(self):\n",
    "          word = '\\0'\n",
    "          self.wv.key_to_index[word] = len(self.wv)\n",
    "          self.wv.index_to_key.append(word)\n",
    "          self.wv.set_vecattr(word, 'count', 1)\n",
    "\n",
    "     def create_binary_tree(self):\n",
    "          _assign_binary_codes(self.wv)\n",
    "\n",
    "     def prepare_vocab(\n",
    "               self, update=False, keep_raw_vocab=False, trim_rule=None\n",
    "          ):\n",
    "          print(\"Preparing vocab...\")\n",
    "          min_count = self.min_count\n",
    "          sample = self.sample\n",
    "          drop_total = drop_unique = 0\n",
    "          \n",
    "          self.effective_min_count = min_count # set effective_min_count to min_count in case max_final_vocab isn't set\n",
    "\n",
    "          new_total = pre_exist_total = 0\n",
    "          new_words = []\n",
    "          pre_exist_words = []\n",
    "\n",
    "\n",
    "\n",
    "          for word, v in self.raw_vocab.items():\n",
    "\n",
    "               effective_min_count = self.effective_min_count\n",
    "               if keep_vocab_item(word, v, effective_min_count):\n",
    "                    if self.wv.has_index_for(word):\n",
    "                         pre_exist_words.append(word)\n",
    "                         pre_exist_total += v\n",
    "\n",
    "                    else:\n",
    "                         new_words.append(word)\n",
    "                         new_total += v\n",
    "                         self.wv.key_to_index[word] = len(self.wv.index_to_key)\n",
    "                         self.wv.index_to_key.append(word)\n",
    "               else:\n",
    "                    drop_unique += 1\n",
    "                    drop_total += v\n",
    "\n",
    "          self.wv.allocate_vecattrs(attrs=['count'], types=[type(0)])\n",
    "          for word in self.wv.index_to_key:\n",
    "               self.wv.set_vecattr(word, 'count', self.wv.get_vecattr(word, 'count') + self.raw_vocab.get(word, 0))\n",
    "     \n",
    "\n",
    "          original_unique_total = len(pre_exist_words) + len(new_words) + drop_unique\n",
    "          pre_exist_unique_pct = len(pre_exist_words) * 100 / max(original_unique_total, 1)\n",
    "          new_unique_pct = len(new_words) * 100 / max(original_unique_total, 1)\n",
    "\n",
    "\n",
    "          retain_words = new_words + pre_exist_words\n",
    "          retain_total = new_total + pre_exist_total\n",
    "\n",
    "\n",
    "\n",
    "          # Precalculate each vocabulary item's threshold for sampling\n",
    "          if not sample:\n",
    "               # no words downsampled\n",
    "               threshold_count = retain_total\n",
    "          elif sample < 1.0:\n",
    "               # traditional meaning: set parameter as proportion of total\n",
    "               threshold_count = sample * retain_total\n",
    "          else:\n",
    "               # new shorthand: sample >= 1 means downsample all words with higher count than sample\n",
    "               threshold_count = int(sample * (3 + np.sqrt(5)) / 2)\n",
    "\n",
    "          downsample_total, downsample_unique = 0, 0\n",
    "          for w in retain_words:\n",
    "               v = self.raw_vocab[w]\n",
    "               word_probability = (np.sqrt(v / threshold_count) + 1) * (threshold_count / v)\n",
    "               if word_probability < 1.0:\n",
    "                    downsample_unique += 1\n",
    "                    downsample_total += word_probability * v\n",
    "               else:\n",
    "                    word_probability = 1.0\n",
    "                    downsample_total += v\n",
    "               \n",
    "               self.wv.set_vecattr(w, 'sample_int', np.uint32(word_probability * (2**32 - 1)))\n",
    "\n",
    "    \n",
    "          self.raw_vocab = defaultdict(int)\n",
    "\n",
    "\n",
    "          # return from each step: words-affected, resulting-corpus-size, extra memory estimates\n",
    "          report_values = {\n",
    "               'drop_unique': drop_unique, 'retain_total': retain_total, 'downsample_unique': downsample_unique,\n",
    "               'downsample_total': int(downsample_total), 'num_retained_words': len(retain_words)\n",
    "          }\n",
    "          print(f\"Report values: {report_values}\")\n",
    "\n",
    "          if self.null_word:\n",
    "            # create null pseudo-word for padding when using concatenative L1 (run-of-words)\n",
    "            # this word is only ever input – never predicted – so count, huffman-point, etc doesn't matter\n",
    "            self.add_null_word()\n",
    "\n",
    "          if self.sorted_vocab and not update:\n",
    "               self.wv.sort_by_descending_frequency()\n",
    "\n",
    "          if self.hs:\n",
    "               # add info about each word's Huffman encoding\n",
    "               self.create_binary_tree()\n",
    "\n",
    "          print(\"negative: \", self.negative)\n",
    "          if self.negative:\n",
    "               # build the table for drawing random words (for negative sampling)\n",
    "               pass\n",
    "          \n",
    "\n",
    "          return report_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     def build_vocab(self, corpus_iterable, corpus_file=None, trim_rule=None, progress_per=10000):\n",
    "          # part 1\n",
    "          \n",
    "          total_words, corpus_count = self.scan_vocab(sentences=corpus_iterable, corpus_file=corpus_file)\n",
    "          print(f\"Total words: {total_words}, Corpus count: {corpus_count}\")\n",
    "          self.corpus_count = corpus_count\n",
    "          self.corpus_total_words = total_words\n",
    "          \n",
    "          # part 2 \n",
    "\n",
    "          report_values = self.prepare_vocab()\n",
    "\n",
    "\n",
    "\n",
    "     def train():\n",
    "          pass\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b078238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 17005207, Corpus count: 1701\n",
      "Preparing vocab...\n",
      "Report values: {'drop_unique': 182564, 'retain_total': 16718844, 'downsample_unique': 38, 'downsample_total': 12506280, 'num_retained_words': 71290}\n",
      "negative:  5\n"
     ]
    }
   ],
   "source": [
    "sentences = CustomLineSentence('data/text8')\n",
    "model = CustomWord2Vec(\n",
    "    sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3080b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Train Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    workers=WORKERS\n",
    ")\n",
    "\n",
    "# 4. Save the trained model\n",
    "model.save('models/text8_w2v_100d.model')\n",
    "\n",
    "# 5. Quick sanity check\n",
    "print(model.wv.most_similar('king', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857f4b7",
   "metadata": {},
   "source": [
    "# We want to use PCA, WHY.\n",
    "\n",
    "PCA (Principal Component Analysis) is used to reduce the dimensionality of high-dimensional data, such as embeddings, while preserving as much of the original information (variance) as possible.\n",
    "\n",
    "We are going to give it `n_components=3` to reduce the embeddings to 3 dimensions.\n",
    "\n",
    "In my test case, we just test the first 100 words.\n",
    "\n",
    "Also, what I can see when I look at the data is that we have a lot of words that are stopwords. We are going to want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e2b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "weights = model.wv.vectors\n",
    "\n",
    "print(weights.shape)  # (number of words, vector size)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce vectors to 3D\n",
    "pca = PCA(n_components=3)\n",
    "weights_3d = pca.fit_transform(weights)\n",
    "\n",
    "# Plot the first 100 words in 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(weights_3d[:100, 0], weights_3d[:100, 1], weights_3d[:100, 2])\n",
    "\n",
    "# Annotate points with words\n",
    "words = list(model.wv.index_to_key)\n",
    "for i in range(100):\n",
    "    ax.text(weights_3d[i, 0], weights_3d[i, 1], weights_3d[i, 2], words[i], size=8)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Word Embeddings in 3D')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
