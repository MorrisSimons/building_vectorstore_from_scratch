{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05882bce",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679f6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall -y -q gensim\n",
    "!pip install -q gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14b45435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "import heapq\n",
    "from collections import defaultdict, namedtuple\n",
    "import numpy as np # we mneed numpy 1.26.4\n",
    "from numpy import float32 as FLOAT_TYPE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9541e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Parameters\n",
    "VECTOR_SIZE = 1000\n",
    "WINDOW      = 5\n",
    "MIN_COUNT   = 5\n",
    "WORKERS     = 4\n",
    "MAX_WORDS_IN_BATCH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78484fb5",
   "metadata": {},
   "source": [
    "### CustomLineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c539367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from gensim import utils\n",
    "\n",
    "class CustomLineSentence:\n",
    "    def __init__(self, source):\n",
    "        self.source = source\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through the lines in the source.\"\"\"\n",
    "        with utils.open(self.source, 'rb') as fin:\n",
    "            for line in itertools.islice(fin, None):\n",
    "                line = utils.to_unicode(line).split()\n",
    "                i = 0\n",
    "                while i < len(line):\n",
    "                    yield line[i: i + MAX_WORDS_IN_BATCH]\n",
    "                    i += MAX_WORDS_IN_BATCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "407a4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 11\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the text8 corpus as a stream of sentences\n",
    "sentences = CustomLineSentence('data/Royal_data.txt')\n",
    "\n",
    "print(\"Number of sentences:\", len(list(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e79c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of 'sentences': <class '__main__.CustomLineSentence'>\n",
      "What we got from 'sentences':\n",
      "Sentence 1: ['The', 'future', 'king', 'is', 'the', 'prince']\n",
      "Length of sentence: 6\n",
      "Sentence 2: ['Daughter', 'is', 'the', 'princess']\n",
      "Length of sentence: 4\n",
      "Sentence 3: ['Son', 'is', 'the', 'prince']\n",
      "Length of sentence: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of 'sentences':\", type(sentences))\n",
    "print(\"What we got from 'sentences':\")\n",
    "for i, s in enumerate(itertools.islice(sentences, 3)):\n",
    "    print(f\"Sentence {i+1}:\", s)\n",
    "\n",
    "    print(\"Length of sentence:\", len(s))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe35c0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in subset: 11\n"
     ]
    }
   ],
   "source": [
    "# limit to the first 100 sentences for downstream processing\n",
    "sentences = list(sentences)[:100]\n",
    "print(\"Number of sentences in subset:\", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b1bb3",
   "metadata": {},
   "source": [
    "# Custom word2vec class\n",
    "by Creating bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ee86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, output_size):\n",
    "        super(BigramModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, embed_size)  # Embedding layer: projects one-hot input to dense vector\n",
    "        self.fc2 = nn.Linear(embed_size, output_size) # Output layer: projects embedding to vocabulary size logits\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)  # Apply softmax to convert logits to probabilities over vocabulary\n",
    "\n",
    "class CustomBigramWord2Vec:\n",
    "    def __init__(self, documents=None, vector_size=100, window=5, min_count=5, epochs=500, workers=4, batch_size=256):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "        self.wv = {}\n",
    "        self.index_to_key = []\n",
    "        \n",
    "        if documents is not None:\n",
    "            self.build_vocab(documents)\n",
    "            self.train_model()\n",
    "    \n",
    "    def build_vocab(self, documents):\n",
    "        \"\"\"Build vocabulary and create bigrams from documents\"\"\"\n",
    "        # Flatten the list of documents into a list of words\n",
    "        all_words = [word for doc in documents for word in doc]\n",
    "        \n",
    "        # Count word occurrences and filter by min_count\n",
    "        word_counts = Counter(all_words)\n",
    "\n",
    "        print(\"word_counts: \", word_counts)\n",
    "\n",
    "        filtered_words = [word for word, count in word_counts.items() if count >= self.min_count]\n",
    "        \n",
    "        # Create word-to-index and index-to-word mappings\n",
    "        self.index_to_key = sorted(filtered_words)\n",
    "\n",
    "        print(\"index_to_key: \", self.index_to_key)\n",
    "\n",
    "        self.word_to_idx = {word: i for i, word in enumerate(self.index_to_key)}\n",
    "        self.vocab_size = len(self.index_to_key)\n",
    "\n",
    "        print(f\"word_to_idx: {self.word_to_idx}\")\n",
    "        \n",
    "        print(f\"Vocabulary size after filtering: {self.vocab_size}\")\n",
    "        \n",
    "        print(\"documents: \", documents)\n",
    "\n",
    "        # Create bigram pairs for training\n",
    "        self.bigrams = []\n",
    "        for doc in documents:\n",
    "            filtered_doc = [word for word in doc if word in self.word_to_idx]\n",
    "            print(\"filtered_doc: \", filtered_doc)\n",
    "            for i in range(len(filtered_doc)):\n",
    "                # Consider words within the window\n",
    "                for j in range(max(0, i - self.window), min(len(filtered_doc), i + self.window + 1)):\n",
    "                    if i != j:  # Skip the word itself\n",
    "                        self.bigrams.append([filtered_doc[i], filtered_doc[j]])\n",
    "        \n",
    "        print(f\"Created {len(self.bigrams)} bigram pairs for training\")\n",
    "\n",
    "        print(\"bigram samples: \", self.bigrams)\n",
    "        \n",
    "        # Create one-hot encoding dictionary\n",
    "        self.onehot_dict = {}\n",
    "        for i, word in enumerate(self.index_to_key):\n",
    "            one_hot = torch.zeros(self.vocab_size)\n",
    "            one_hot[i] = 1\n",
    "            self.onehot_dict[word] = one_hot\n",
    "            \n",
    "        # Prepare training data\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        for bi in self.bigrams:\n",
    "            if bi[0] in self.onehot_dict and bi[1] in self.onehot_dict:\n",
    "                self.X.append(self.onehot_dict[bi[0]])\n",
    "                self.Y.append(self.onehot_dict[bi[1]])\n",
    "                \n",
    "        self.X = torch.stack(self.X)\n",
    "        self.Y_indices = torch.tensor([self.word_to_idx[bi[1]] for bi in self.bigrams \n",
    "                                      if bi[0] in self.onehot_dict and bi[1] in self.onehot_dict], \n",
    "                                     dtype=torch.long)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"Train the bigram model\"\"\"\n",
    "        self.model = BigramModel(self.vocab_size, self.vector_size, self.vocab_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters())\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            permutation = torch.randperm(self.X.size()[0])\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, self.X.size()[0], self.batch_size):\n",
    "                indices = permutation[i:i+self.batch_size]\n",
    "                batch_x, batch_y = self.X[indices], self.Y_indices[indices]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Extract word embeddings from the trained model\n",
    "        weights = self.model.fc1.weight.data.T.numpy()  # shape: (vocab_size, vector_size)\n",
    "        for i, word in enumerate(self.index_to_key):\n",
    "            self.wv[word] = weights[i]\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Save model to disk\"\"\"\n",
    "        data = {\n",
    "            'wv': self.wv,\n",
    "            'index_to_key': self.index_to_key,\n",
    "            'vector_size': self.vector_size,\n",
    "            'window': self.window,\n",
    "            'min_count': self.min_count,\n",
    "        }\n",
    "        torch.save(data, path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load model from disk\"\"\"\n",
    "        data = torch.load(path)\n",
    "        model = cls()\n",
    "        model.wv = data['wv']\n",
    "        model.index_to_key = data['index_to_key']\n",
    "        model.vector_size = data['vector_size']\n",
    "        model.window = data['window']\n",
    "        model.min_count = data['min_count']\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a7d88",
   "metadata": {},
   "source": [
    "## Custom Doc2Vec Class\n",
    "Use Gensim's Doc2Vec and TaggedDocument to build and use document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "655094f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import pickle\n",
    "import pickle\n",
    "\n",
    "class CustomDoc2Vec:\n",
    "    def __init__(self, documents, vector_size=100, window=5, min_count=5, epochs=5, workers=4, negative=5):\n",
    "        self.vector_size = vector_size\n",
    "        self.documents = documents\n",
    "        self.doc_vectors = np.zeros((len(documents), vector_size), dtype=FLOAT_TYPE)\n",
    "        \n",
    "        # Train Word2Vec on the documents\n",
    "        self.word_model = CustomBigramWord2Vec(\n",
    "            documents=documents,\n",
    "            vector_size=vector_size,\n",
    "            window=window,\n",
    "            min_count=min_count,\n",
    "            workers=workers,\n",
    "            epochs=epochs\n",
    "        )\n",
    "        \n",
    "        # Generate document vectors by averaging word vectors\n",
    "        for i, doc in enumerate(documents):\n",
    "            valid_words = [word for word in doc if word in self.word_model.wv]\n",
    "            if valid_words:\n",
    "                self.doc_vectors[i] = np.mean([self.word_model.wv[word] for word in valid_words], axis=0)\n",
    "    \n",
    "    def infer_vector(self, document):\n",
    "        \"\"\"\n",
    "        Infer a vector for a new document by averaging its word vectors\n",
    "        \"\"\"\n",
    "        valid_words = [word for word in document if word in self.word_model.wv]\n",
    "        if not valid_words:\n",
    "            return np.zeros(self.vector_size, dtype=FLOAT_TYPE)\n",
    "        return np.mean([self.word_model.wv[word] for word in valid_words], axis=0)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model to file\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'word_model': self.word_model,\n",
    "                'doc_vectors': self.doc_vectors,\n",
    "                'vector_size': self.vector_size,\n",
    "                'documents': self.documents\n",
    "            }, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load model from file\"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        obj = cls.__new__(cls)\n",
    "        obj.word_model = data['word_model']\n",
    "        obj.doc_vectors = data['doc_vectors']\n",
    "        obj.vector_size = data['vector_size']\n",
    "        obj.documents = data['documents']\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37f055",
   "metadata": {},
   "source": [
    "## Document Vector Store\n",
    "Build document embeddings and search functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34f934be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "\n",
    "class DocumentVectorStore:\n",
    "    def __init__(self, documents, embedding_model=None):\n",
    "        \"\"\"documents: list of token lists or raw text strings\"\"\"\n",
    "        self.raw_documents = documents\n",
    "        self.filter_stopwords = True\n",
    "\n",
    "\n",
    "        # get stopwords rom nltk\n",
    "        if self.filter_stopwords:\n",
    "            try:\n",
    "                self.stopwords = set(stopwords.words('english'))\n",
    "            except:\n",
    "                print(\"downloading stopwords...\")\n",
    "                nltk.download('stopwords')\n",
    "                self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "        # tokenize \n",
    "        self.tokenized_docs = [doc.split() if isinstance(doc, str) else doc for doc in documents]\n",
    "\n",
    "        # If stopword filtering is enabled, process each document\n",
    "        if self.filter_stopwords:\n",
    "            processed = []  # initialize list to hold processed token lists\n",
    "            for doc in self.raw_documents: \n",
    "                # split raw text into tokens if it's a string, otherwise use the list as is\n",
    "                tokens = doc.split() if isinstance(doc, str) else doc\n",
    "                # filter out tokens that are in the stopword set (case-insensitive)\n",
    "                processed.append([tok for tok in tokens if tok.lower() not in self.stopwords])\n",
    "                self.tokenized_docs = processed  # assign the filtered token lists\n",
    "        \n",
    "        self.model = embedding_model\n",
    "        # build document embeddings\n",
    "        self.build_doc_embeddings()\n",
    "\n",
    "    def build_doc_embeddings(self):\n",
    "        \"\"\"Compute document embeddings by averaging word vectors\"\"\"\n",
    "        embeddings = []\n",
    "        print(\"model\", self.model)\n",
    "        for tokens in self.tokenized_docs:\n",
    "            vecs = [self.model.wv[word] for word in tokens if word in self.model.wv]\n",
    "            if vecs:\n",
    "                doc_vec = np.mean(vecs, axis=0)\n",
    "            else:\n",
    "                doc_vec = np.zeros(self.model.vector_size)\n",
    "            embeddings.append(doc_vec)\n",
    "        self.doc_embeddings = np.vstack(embeddings)\n",
    "    \n",
    "    def search(self, query, topn=5):\n",
    "        \"\"\"Return topn most similar documents for a query (tokens or string)\"\"\"\n",
    "        if isinstance(query, str):\n",
    "            tokens = query.split()\n",
    "        else:\n",
    "            tokens = query\n",
    "        \n",
    "        # Filter stopwords if enabled\n",
    "        if self.filter_stopwords:\n",
    "            tokens = [word for word in tokens if word.lower() not in self.stopwords]\n",
    "\n",
    "        # Define vecs before using it\n",
    "        vecs = [self.model.wv[word] for word in tokens if word in self.model.wv]\n",
    "        \n",
    "        if vecs:\n",
    "            q_vec = np.mean(vecs, axis=0)\n",
    "        else:\n",
    "            q_vec = np.zeros(self.model.vector_size)\n",
    "            \n",
    "        # cosine similarity\n",
    "        sims = self.doc_embeddings.dot(q_vec) / (norm(self.doc_embeddings, axis=1) * norm(q_vec) + 1e-9)\n",
    "        idx = np.argsort(-sims)[:topn]\n",
    "        \n",
    "        return [(i, float(sims[i])) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e2138411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 11\n",
      "index_to_key:  ['a', 'be', 'is']\n",
      "word_to_idx: {'a': 0, 'be': 1, 'is': 2}\n",
      "Vocabulary size after filtering: 3\n",
      "documents:  [['The', 'future', 'king', 'is', 'the', 'prince'], ['Daughter', 'is', 'the', 'princess'], ['Son', 'is', 'the', 'prince'], ['Only', 'a', 'man', 'can', 'be', 'a', 'king'], ['Only', 'a', 'woman', 'can', 'be', 'a', 'queen'], ['The', 'princess', 'will', 'be', 'a', 'queen'], ['The', 'prince', 'is', 'a', 'strong', 'man'], ['The', 'princess', 'is', 'a', 'beautiful', 'woman'], ['Prince', 'is', 'only', 'a', 'boy', 'now'], ['Prince', 'will', 'be', 'king'], ['A', 'boy', 'will', 'be', 'a', 'man']]\n",
      "filtered_doc:  ['is']\n",
      "filtered_doc:  ['is']\n",
      "filtered_doc:  ['is']\n",
      "filtered_doc:  ['a', 'be', 'a']\n",
      "filtered_doc:  ['a', 'be', 'a']\n",
      "filtered_doc:  ['be', 'a']\n",
      "filtered_doc:  ['is', 'a']\n",
      "filtered_doc:  ['is', 'a']\n",
      "filtered_doc:  ['is', 'a']\n",
      "filtered_doc:  ['be']\n",
      "filtered_doc:  ['be', 'a']\n",
      "Created 22 bigram pairs for training\n",
      "bigram samples:  [['a', 'be'], ['a', 'a'], ['be', 'a'], ['be', 'a'], ['a', 'a'], ['a', 'be'], ['a', 'be'], ['a', 'a'], ['be', 'a'], ['be', 'a'], ['a', 'a'], ['a', 'be'], ['be', 'a'], ['a', 'be'], ['is', 'a'], ['a', 'is'], ['is', 'a'], ['a', 'is'], ['is', 'a'], ['a', 'is'], ['be', 'a'], ['a', 'be']]\n",
      "Epoch 100/1000, Loss: 0.8508\n",
      "Epoch 200/1000, Loss: 0.8508\n",
      "Epoch 300/1000, Loss: 0.8508\n",
      "Epoch 400/1000, Loss: 0.8508\n",
      "Epoch 500/1000, Loss: 0.8507\n",
      "Epoch 600/1000, Loss: 0.8507\n",
      "Epoch 700/1000, Loss: 0.8507\n",
      "Epoch 800/1000, Loss: 0.8507\n",
      "Epoch 900/1000, Loss: 0.8507\n",
      "Epoch 1000/1000, Loss: 0.8507\n",
      "model <__main__.CustomBigramWord2Vec object at 0x74a864c79960>\n",
      "Top 5 similar documents (index, similarity): [(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of documents:\", len(sentences))\n",
    "\n",
    "# Train our CustomDoc2Vec model\n",
    "doc2vec_model = CustomDoc2Vec(\n",
    "    documents=sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW,\n",
    "    min_count=MIN_COUNT,\n",
    "    epochs=1000,\n",
    "    workers=WORKERS,\n",
    "    negative=5\n",
    ")\n",
    "\n",
    "# Save and reload the Doc2Vec model\n",
    "doc2vec_model.save('./models/custom_doc2vec.model')\n",
    "loaded_doc2vec = CustomDoc2Vec.load('./models/custom_doc2vec.model')\n",
    "\n",
    "# Build a DocumentVectorStore using the trained Doc2Vec embeddings\n",
    "doc_store = DocumentVectorStore(sentences, embedding_model=loaded_doc2vec.word_model)\n",
    "\n",
    "# testing search\n",
    "query_doc = sentences[0]\n",
    "results = doc_store.search(query_doc, topn=5)\n",
    "print(\"Top 5 similar documents (index, similarity):\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff81b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0a0b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daughter is the princess\n"
     ]
    }
   ],
   "source": [
    "query_doc = sentences[1]\n",
    "# join all words into one long string\n",
    "text = \" \".join(query_doc)\n",
    "\n",
    "# wrap at 80 chars per line\n",
    "print(textwrap.fill(text, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af38ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchterm = \"\"\"experience however in practice many autistic people have difficulty with working\n",
    "in groups which impairs them even in the most technical of situations autistic\n",
    "adults temple grandin one of the more successful adults with autism photograph\n",
    "courtesy joshua nathaniel pritikin and william lawrence jarrold some autistic\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87a7659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_search_term = \"king\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ed618fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar documents for king, (index, similarity): [(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0), (9, 0.0), (10, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "results = doc_store.search(sample_search_term, topn=20)\n",
    "print(f\"Top 5 similar documents for {sample_search_term}, (index, similarity):\", results,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e65cee7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daughter is the princess\n"
     ]
    }
   ],
   "source": [
    "# join all words into one long string\n",
    "query_doc = sentences[1]\n",
    "text = \" \".join(query_doc)\n",
    "\n",
    "# wrap at 80 chars per line\n",
    "print(textwrap.fill(text, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857f4b7",
   "metadata": {},
   "source": [
    "# We want to use PCA, WHY.\n",
    "\n",
    "PCA (Principal Component Analysis) is used to reduce the dimensionality of high-dimensional data, such as embeddings, while preserving as much of the original information (variance) as possible.\n",
    "\n",
    "We are going to give it `n_components=3` to reduce the embeddings to 3 dimensions.\n",
    "\n",
    "In my test case, we just test the first 100 words.\n",
    "\n",
    "Also, what I can see when I look at the data is that we have a lot of words that are stopwords. We are going to want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15e2b1bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomDoc2Vec' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m numOfWords \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 5\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mdoc2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(weights\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (number of words, vector size)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomDoc2Vec' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numOfWords = 10\n",
    "\n",
    "weights = doc2vec_model.model.wv.vectors\n",
    "\n",
    "print(weights.shape)  # (number of words, vector size)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce vectors to 3D\n",
    "pca = PCA(n_components=3)\n",
    "weights_3d = pca.fit_transform(weights)\n",
    "\n",
    "# Plot the first 1000 words in 3D\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(weights_3d[:numOfWords, 0], weights_3d[:numOfWords, 1], weights_3d[:numOfWords, 2])\n",
    "\n",
    "# Annotate points with words\n",
    "words = list(doc2vec_model.model.wv.index_to_key)\n",
    "for i in range(numOfWords):\n",
    "    ax.text(weights_3d[i, 0], weights_3d[i, 1], weights_3d[i, 2], words[i], size=8)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Word Embeddings in 3D')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce document embeddings to 3D and plot them\n",
    "\n",
    "# Get document embeddings from doc_store\n",
    "doc_embeddings = doc_store.doc_embeddings\n",
    "\n",
    "print(doc_embeddings.shape)  # (number of documents, vector size)\n",
    "\n",
    "# Reduce to 3D\n",
    "pca = PCA(n_components=3)\n",
    "doc_emb_3d = pca.fit_transform(doc_embeddings)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(doc_emb_3d[:100, 0], doc_emb_3d[:100, 1], doc_emb_3d[:100, 2])\n",
    "\n",
    "# Annotate points with document indices\n",
    "for i in range(100):\n",
    "    ax.text(doc_emb_3d[i, 0], doc_emb_3d[i, 1], doc_emb_3d[i, 2], str(i), size=8)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.title('Document Embeddings in 3D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227fe503",
   "metadata": {},
   "source": [
    "# Evaluation of Vector Store Solution\n",
    "\n",
    "Let's compare our custom vector store solution with other standard approaches to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
